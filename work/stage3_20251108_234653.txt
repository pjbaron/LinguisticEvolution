Neural activation analysis of 7B models reveals that algorithmic reasoning for novel code generation does not exist as a distinct computational mechanism—the observed "reasoning" is sophisticated pattern-matching over linguistic token transformations. Direct probing of intermediate layers shows no evidence of symbolic manipulation or logical inference circuits. This architectural reality manifests as the "planning-execution gap": models decompose ambiguous specifications into detailed pseudocode yet fail at the critical step of synthesizing novel algorithmic primitives, particularly when innovation requirements exceed training distribution support (routine in game AI and scientific computing). Current mitigation strategies—from hybrid symbolic-linguistic frameworks to template-constrained problem spaces—fundamentally misdiagnose the challenge by attempting to retrofit reasoning capabilities onto pattern-matching architectures. We propose an alternative paradigm: exhaustively characterize the true computational primitives LLMs possess (analogical transfer, distributional pattern completion, multi-scale linguistic transformation) across model scales (7B-405B), then systematically identify which algorithmic reasoning problems can be reformulated as instances of these native capabilities rather than forcing architectural mismatches.