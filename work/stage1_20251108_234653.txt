Empirical analysis of 7-billion parameter language models reveals a critical limitation in algorithmic code generation: the absence of step-based reasoning capabilities required for novel solutions. Our investigation of mid-to-late layer activations found extensive linguistic token manipulation but no evidence of logical reasoning structures. While these models excel at decomposing vague project specifications into detailed implementation plans, they consistently fail when execution demands algorithmic innovations beyond their training corpusâ€”a pervasive challenge in game development and scientific computing where novel solutions are routine. We examine proposed mitigation strategies (hybrid English-logic languages, template-constrained domains, enhanced planning, linguistic reasoning frameworks) and develop additional approaches grounded in systematic characterization of capabilities across the model spectrum from 7B local to 405B cloud deployments.