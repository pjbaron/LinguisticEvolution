Activation-level analysis of 7B parameter models exposes a fundamental architectural constraint: algorithmic reasoning for novel code generation is absent, replaced by sophisticated but ultimately pattern-matching token transformations. Our neural probe of intermediate and deep layers revealed no reasoning circuitry—only linguistic feature manipulation. This creates a "planning-execution gap": models produce meticulous decompositions of underspecified projects yet cannot synthesize the algorithmic primitives needed for implementation when those primitives lie outside training distributions. This gap is not merely inconvenient but represents a critical bottleneck in domains requiring frequent innovation (game AI, scientific simulation). Existing workarounds—hybrid symbolic-natural languages, template-based domain restriction, recursive planning, language-mediated reasoning—address symptoms rather than causes. We propose reframing the problem: instead of forcing LLMs to reason algorithmically, we systematically map their true capabilities (pattern completion, structural analogy, linguistic transformation) across scales (7B-405B) to identify which reasoning tasks can be recast as language tasks they naturally solve.