Adversarial examples—inputs deliberately designed to fool machine learning models—reveal vulnerabilities where imperceptible perturbations cause dramatic misclassifications. These failures indicate models learn superficial patterns rather than robust representations and pose security risks for deployed systems. Developing adversarially robust models requires training techniques that explicitly account for worst-case perturbations and better understanding of what makes models vulnerable.