In moral reasoning systems, it's logically easier to reach conclusions about what we shouldn't do ("ought not") than what we should do ("ought to"). This happens because preventing harm only requires identifying bad outcomes and blocking them, while prescribing action requires additional justification for why specific behaviors should be required. This structural feature helps explain why moral communities more easily agree on shared taboos (things we all agree not to do) than on shared positive duties (things we all agree we must do).