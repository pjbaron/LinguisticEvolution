This proposition argues that standard distributed training of CNNs creates architectural mismatch by treating interdependent spatially-related filters as independent variables during updates. When asynchronous gradient updates cause spatially-related filters to diverge across nodes, the network's feature hierarchy degrades as lower-level edge detectors become misaligned with higher-level pattern recognizers. The solution requires filter-aware distributed training preserving spatial coherence through regularization terms maintaining correlations between related filters and parameter averaging schemes respecting hierarchical dependencies.