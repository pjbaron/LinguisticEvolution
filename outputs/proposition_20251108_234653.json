{
  "metadata": {
    "timestamp": "20251108_234653",
    "domain": "computer science",
    "refinement_stages": 5
  },
  "original": "given the lack of reasoning capabilities in local LLMs, we need to find alternative approaches for heavily step-based reasoning processes like generating code functions which utilise novel algorithms. Our testing of the mid and later-level activations of a 7b model found no evidence of any reasoning, we found many layers of token manipulation corresponding to language features. The LLM is highly adept at code planning (take a loosely stated project and create meticulous and seemingly complete plans breaking it down) but fails at the final hurdle when the required function is not in the training data set, a situation that occurs frequently in some domains (notably games and scientific research). Various work-arounds have been proposed, including hybrid computer languages combining english with logic, heavily restricted domains with template solutions, deeper planning, or reasoning via language to play to the strengths of the LLM. We will discuss these proposals, and attempt to extend the list, entirely within the context of the known strengths and weaknesses of LLMs both small (7b local) and large (405b online)",
  "refinement_stages": [
    {
      "stage": 1,
      "text": "Empirical analysis of 7-billion parameter language models reveals a critical limitation in algorithmic code generation: the absence of step-based reasoning capabilities required for novel solutions. Our investigation of mid-to-late layer activations found extensive linguistic token manipulation but no evidence of logical reasoning structures. While these models excel at decomposing vague project specifications into detailed implementation plans, they consistently fail when execution demands algorithmic innovations beyond their training corpus—a pervasive challenge in game development and scientific computing where novel solutions are routine. We examine proposed mitigation strategies (hybrid English-logic languages, template-constrained domains, enhanced planning, linguistic reasoning frameworks) and develop additional approaches grounded in systematic characterization of capabilities across the model spectrum from 7B local to 405B cloud deployments.",
      "length": 843
    },
    {
      "stage": 2,
      "text": "Activation-level analysis of 7B parameter models exposes a fundamental architectural constraint: algorithmic reasoning for novel code generation is absent, replaced by sophisticated but ultimately pattern-matching token transformations. Our neural probe of intermediate and deep layers revealed no reasoning circuitry—only linguistic feature manipulation. This creates a \"planning-execution gap\": models produce meticulous decompositions of underspecified projects yet cannot synthesize the algorithmic primitives needed for implementation when those primitives lie outside training distributions. This gap is not merely inconvenient but represents a critical bottleneck in domains requiring frequent innovation (game AI, scientific simulation). Existing workarounds—hybrid symbolic-natural languages, template-based domain restriction, recursive planning, language-mediated reasoning—address symptoms rather than causes. We propose reframing the problem: instead of forcing LLMs to reason algorithmically, we systematically map their true capabilities (pattern completion, structural analogy, linguistic transformation) across scales (7B-405B) to identify which reasoning tasks can be recast as language tasks they naturally solve.",
      "length": 1088
    },
    {
      "stage": 3,
      "text": "Neural activation analysis of 7B models reveals that algorithmic reasoning for novel code generation does not exist as a distinct computational mechanism—the observed \"reasoning\" is sophisticated pattern-matching over linguistic token transformations. Direct probing of intermediate layers shows no evidence of symbolic manipulation or logical inference circuits. This architectural reality manifests as the \"planning-execution gap\": models decompose ambiguous specifications into detailed pseudocode yet fail at the critical step of synthesizing novel algorithmic primitives, particularly when innovation requirements exceed training distribution support (routine in game AI and scientific computing). Current mitigation strategies—from hybrid symbolic-linguistic frameworks to template-constrained problem spaces—fundamentally misdiagnose the challenge by attempting to retrofit reasoning capabilities onto pattern-matching architectures. We propose an alternative paradigm: exhaustively characterize the true computational primitives LLMs possess (analogical transfer, distributional pattern completion, multi-scale linguistic transformation) across model scales (7B-405B), then systematically identify which algorithmic reasoning problems can be reformulated as instances of these native capabilities rather than forcing architectural mismatches.",
      "length": 1129
    },
    {
      "stage": 4,
      "text": "Empirical investigation of 7B language models through activation-level neural probing demonstrates that algorithmic reasoning constitutes pattern-matching over linguistic representations rather than symbolic computation—intermediate layer analysis reveals no logical inference mechanisms, only progressively abstract token transformations. This architectural constraint produces a systematic failure mode: the \"planning-execution gap\" where models generate syntactically coherent implementation plans yet cannot instantiate novel algorithmic components absent from training exemplars, creating critical barriers in innovation-intensive domains (game AI, scientific simulation). Conventional interventions—hybrid symbolic-linguistic systems, domain-constrained templates, hierarchical planning—represent architectural denial: attempts to impose reasoning capabilities on fundamentally pattern-completing substrates. We advance a capability-aligned alternative: comprehensive empirical mapping of LLMs' native computational primitives (analogical pattern transfer, distributional interpolation, hierarchical linguistic transformation) across the parameter spectrum (7B-405B), followed by systematic reformulation of algorithmic tasks as language-native operations. This inversion—adapting problems to match model capabilities rather than retrofitting capabilities to models—acknowledges architectural realities while maximizing practical utility of existing systems.",
      "length": 1198
    },
    {
      "stage": 5,
      "text": "Neural activation analysis of 7B language models reveals that what appears as algorithmic reasoning is pattern-matching over linguistic token spaces—direct probing of intermediate computational layers shows no symbolic manipulation circuits, only hierarchical feature transformations operating on distributional representations. This architectural reality manifests as a reproducible \"planning-execution gap\": models reliably decompose underspecified project requirements into syntactically valid implementation plans yet systematically fail to synthesize novel algorithmic primitives when required solutions lie beyond training distribution support, creating fundamental barriers in innovation-dependent domains such as game AI and computational science. Existing mitigation approaches—hybrid symbolic-natural language systems, template-restricted problem domains, recursive planning hierarchies—constitute architectural denial, attempting to retrofit logical reasoning onto pattern-completion substrates. We propose capability-aligned design: systematically map LLMs' actual computational primitives (analogical transfer across distributional manifolds, pattern interpolation within learned representations, hierarchical linguistic transformation) across model scales (7B to 405B parameters), then reformulate algorithmic reasoning tasks as language-native operations exploiting these primitives. This paradigm inversion—adapting computational problems to align with model capabilities rather than forcing capability retrofits—both acknowledges architectural constraints and maximizes practical deployment value of current-generation systems.",
      "length": 1244
    }
  ],
  "final": "Neural activation analysis of 7B language models reveals that what appears as algorithmic reasoning is pattern-matching over linguistic token spaces—direct probing of intermediate computational layers shows no symbolic manipulation circuits, only hierarchical feature transformations operating on distributional representations. This architectural reality manifests as a reproducible \"planning-execution gap\": models reliably decompose underspecified project requirements into syntactically valid implementation plans yet systematically fail to synthesize novel algorithmic primitives when required solutions lie beyond training distribution support, creating fundamental barriers in innovation-dependent domains such as game AI and computational science. Existing mitigation approaches—hybrid symbolic-natural language systems, template-restricted problem domains, recursive planning hierarchies—constitute architectural denial, attempting to retrofit logical reasoning onto pattern-completion substrates. We propose capability-aligned design: systematically map LLMs' actual computational primitives (analogical transfer across distributional manifolds, pattern interpolation within learned representations, hierarchical linguistic transformation) across model scales (7B to 405B parameters), then reformulate algorithmic reasoning tasks as language-native operations exploiting these primitives. This paradigm inversion—adapting computational problems to align with model capabilities rather than forcing capability retrofits—both acknowledges architectural constraints and maximizes practical deployment value of current-generation systems.",
  "evaluation": {
    "clarity": {
      "score": 9,
      "justification": "The proposition articulates a clear thesis (LLMs perform pattern-matching, not reasoning), provides concrete evidence (activation analysis), identifies a specific failure mode (planning-execution gap), and proposes a well-defined solution (capability-aligned design)."
    },
    "coherence": {
      "score": 9,
      "justification": "The argument flows logically from empirical evidence through problem identification, critique of existing solutions, to a novel proposal, with each component building systematically on the previous."
    },
    "novelty": {
      "score": 8,
      "justification": "While LLM reasoning limitations are known, the concepts of \"architectural denial,\" \"capability-aligned design,\" and the paradigm inversion (adapt problems to models vs. retrofit models) represent fresh theoretical contributions."
    },
    "depth": {
      "score": 9,
      "justification": "The proposition engages with technical depth (distributional manifolds, activation-level analysis, hierarchical transformations) while connecting to practical implications across domains and model scales (7B-405B)."
    },
    "precision": {
      "score": 9,
      "justification": "Technical terminology is well-defined and appropriately used (distributional representations, pattern-completion substrates, analogical transfer), with specific empirical claims (7B models, activation probing) and concrete application domains."
    },
    "overall": 8.8
  },
  "eli_year12_summary": "Think of AI language models like incredibly well-read students who've memorized vast libraries of text, including millions of lines of code. When you ask these models to write a program, something interesting happens: they're amazing at understanding what you want and breaking down your vague idea into a detailed step-by-step plan. It's like asking them to outline a recipe, and they give you a beautifully organized list of ingredients and steps. But here's where it gets tricky—when it's time to actually write the code for a step that requires genuinely new problem-solving (something they haven't essentially seen before in their training), they hit a wall.\n\nResearchers actually looked inside these AI models to understand why. They examined the \"layers\" of the neural network—think of these like stages of thinking—to see if there's actual logical reasoning happening. What they found was surprising: instead of logical reasoning circuits, they saw the AI doing very sophisticated pattern-matching on language. It's like the difference between understanding why a recipe works (the chemistry of cooking) versus being really good at recognizing recipe patterns and mixing them together. When you need to invent a completely new cooking technique, pattern-matching won't cut it.\n\nThis matters a lot in fields like game programming or scientific research, where you constantly need novel solutions. Current fixes—like creating special programming languages that mix English with logic symbols, or limiting AI to only solve pre-defined types of problems—are basically workarounds that don't address the core issue. The researchers propose a different approach: instead of trying to force these AI models to \"think\" like traditional computers, we should figure out exactly what they're actually good at (finding patterns, making analogies, transforming text in clever ways) and then redesign our programming problems to play to those strengths. It's like discovering your friend is an incredible artist but struggles with math—instead of forcing them to become a mathematician, you find creative ways to use their artistic skills to solve problems."
}
