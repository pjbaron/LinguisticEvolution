This proposition proposes organizing tensor data in square blocks that align with 2D convolution access patterns rather than traditional row-major storage. By clustering pixels accessed within a convolution window into contiguous memory, this approach transforms irregular sliding window access into coalesced reads that align with GPU warp execution, potentially improving memory bandwidth utilization by up to 3x. However, optimal block dimensions require careful tuning based on cache characteristics and tensor shapes, with suboptimal sizes introducing padding overhead that can negate spatial locality benefits.